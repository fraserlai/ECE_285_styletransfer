{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.tensor\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "import copy\n",
    "import utils\n",
    "from net import Net, Vgg16\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'evaluate'#'train'\n",
    "\n",
    "# ----- fixed -----\n",
    "DATASET_FOLDER = 'dataset/'\n",
    "STYLE_FOLDER = 'images/9styles/'\n",
    "SAVE_MODEL_DIR = 'models/'\n",
    "VGG_DIR = 'models/'\n",
    "# ----- fixed -----\n",
    "\n",
    "# ----- training parameters -----\n",
    "EPOCHS = 10\n",
    "IMAGE_SIZE = 256\n",
    "FILTER_CHANNEL = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZE = 4\n",
    "CONT_WEIGHT = 1e-3\n",
    "STYLE_WEIGHT = 1.0\n",
    "RESUME = None #''\n",
    "# ----- training parameters -----\n",
    "\n",
    "# ----- evaluate parameters -----\n",
    "EV_MODEL_DIR = 'models/Final_epoch_10_Mon_Dec_10_01:47:17_2018_0.001_1.0.model'\n",
    "EV_CONT_IMG = 'images/dancing.jpg'\n",
    "EV_STYLE_IMG = STYLE_FOLDER + 'wave.jpg'\n",
    "EV_OUTPUT_IMG = 'output/wave_Final_epoch_10_Mon_Dec_10_01:47:17_2018_0.001_1.0.model.png'\n",
    "# ----- evaluate parameters -----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Device (CPU/GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    check_point_path = ''\n",
    "\n",
    "    transform = transforms.Compose([transforms.Scale(IMAGE_SIZE),\n",
    "                                    transforms.CenterCrop(IMAGE_SIZE),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Lambda(lambda x: x.mul(255))])\n",
    "\n",
    "    train_dataset = datasets.ImageFolder(DATASET_FOLDER, transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "    style_model = Net(ngf=FILTER_CHANNEL, dv=device).to(device)\n",
    "    if RESUME is not None:\n",
    "        print('Resuming, initializing using weight from {}.'.format(RESUME))\n",
    "        style_model.load_state_dict(torch.load(RESUME))\n",
    "    print(style_model)\n",
    "    optimizer = Adam(style_model.parameters(), LEARNING_RATE)\n",
    "    mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "    vgg = Vgg16()\n",
    "    utils.init_vgg16(VGG_DIR)\n",
    "    vgg.load_state_dict(torch.load(os.path.join(VGG_DIR, \"vgg16.weight\")))\n",
    "    vgg.to(device)\n",
    "\n",
    "    style_loader = utils.StyleLoader(STYLE_FOLDER, IMAGE_SIZE, device)\n",
    "    \n",
    "    tbar = tqdm(range(EPOCHS))\n",
    "    for e in tbar:\n",
    "        style_model.train()\n",
    "        agg_content_loss = 0.\n",
    "        agg_style_loss = 0.\n",
    "        count = 0\n",
    "        for batch_id, (x, _) in enumerate(train_loader):\n",
    "            n_batch = len(x)\n",
    "            count += n_batch\n",
    "            optimizer.zero_grad()\n",
    "            x = Variable(utils.preprocess_batch(x)).to(device)\n",
    "\n",
    "            style_v = style_loader.get(batch_id)\n",
    "            style_model.setTarget(style_v)\n",
    "\n",
    "            style_v = utils.subtract_imagenet_mean_batch(style_v, device)\n",
    "            features_style = vgg(style_v)\n",
    "            gram_style = [utils.gram_matrix(y) for y in features_style]\n",
    "\n",
    "            y = style_model(x)\n",
    "            xc = Variable(x.data.clone())\n",
    "\n",
    "            y = utils.subtract_imagenet_mean_batch(y, device)\n",
    "            xc = utils.subtract_imagenet_mean_batch(xc, device)\n",
    "\n",
    "            features_y = vgg(y)\n",
    "            features_xc = vgg(xc)\n",
    "\n",
    "            f_xc_c = Variable(features_xc[1].data, requires_grad=False)\n",
    "\n",
    "            content_loss = CONT_WEIGHT * mse_loss(features_y[1], f_xc_c)\n",
    "\n",
    "            style_loss = 0.\n",
    "            for m in range(len(features_y)):\n",
    "                gram_y = utils.gram_matrix(features_y[m])\n",
    "                gram_s = Variable(gram_style[m].data, requires_grad=False).repeat(BATCH_SIZE, 1, 1, 1)\n",
    "                style_loss += STYLE_WEIGHT * mse_loss(gram_y.unsqueeze_(1), gram_s[:n_batch, :, :])\n",
    "\n",
    "            total_loss = content_loss + style_loss\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            agg_content_loss += content_loss.data[0]\n",
    "            agg_style_loss += style_loss.data[0]\n",
    "\n",
    "            if (batch_id + 1) % 100 == 0:\n",
    "                mesg = \"{}\\tEpoch {}:\\t[{}/{}]\\tcontent: {:.6f}\\tstyle: {:.6f}\\ttotal: {:.6f}\".format(\n",
    "                    time.ctime(), e + 1, count, len(train_dataset),\n",
    "                                agg_content_loss / (batch_id + 1),\n",
    "                                agg_style_loss / (batch_id + 1),\n",
    "                                (agg_content_loss + agg_style_loss) / (batch_id + 1)\n",
    "                )\n",
    "                tbar.set_description(mesg)\n",
    "\n",
    "            \n",
    "            if (batch_id + 1) % (4 * 100) == 0:\n",
    "                # save model\n",
    "                style_model.eval()\n",
    "                style_model.cpu()\n",
    "                save_model_filename = \"Epoch_\" + str(e) + \"iters_\" + str(count) + \"_\" + \\\n",
    "                    str(time.ctime()).replace(' ', '_') + \"_\" + str(\n",
    "                    CONT_WEIGHT) + \"_\" + str(STYLE_WEIGHT) + \".model\"\n",
    "                save_model_path = os.path.join(SAVE_MODEL_DIR, save_model_filename)\n",
    "                torch.save(style_model.state_dict(), save_model_path)\n",
    "                if check_point_path:\n",
    "                    os.remove(check_point_path)\n",
    "                check_point_path = save_model_path\n",
    "                style_model.train()\n",
    "                style_model.cuda()\n",
    "                tbar.set_description(\"\\nCheckpoint, trained model saved at\", save_model_path)\n",
    "\n",
    "    # save model\n",
    "    style_model.eval()\n",
    "    style_model.cpu()\n",
    "    save_model_filename = \"Final_epoch_\" + str(EPOCHS) + \"_\" + \\\n",
    "        str(time.ctime()).replace(' ', '_') + \"_\" + str(\n",
    "        CONT_WEIGHT) + \"_\" + str(STYLE_WEIGHT) + \".model\"\n",
    "    save_model_path = os.path.join(SAVE_MODEL_DIR, save_model_filename)\n",
    "    torch.save(style_model.state_dict(), save_model_path)\n",
    "    if check_point_path:\n",
    "        os.remove(check_point_path)\n",
    "\n",
    "    print(\"\\nDone, trained model saved at\", save_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model_dir, c_img, s_img, img_size, out_img):\n",
    "    content_image = utils.tensor_load_rgbimage(c_img, size=img_size, keep_asp=True)\n",
    "    content_image = content_image.unsqueeze(0).to(device)\n",
    "    style = utils.tensor_load_rgbimage(s_img, size=img_size)\n",
    "    style = style.unsqueeze(0)    \n",
    "    style = utils.preprocess_batch(style).to(device)\n",
    "\n",
    "    style_model = Net(ngf=FILTER_CHANNEL, dv=device).to(device)\n",
    "    style_model.load_state_dict(torch.load(model_dir), False)\n",
    "    \n",
    "    style_v = Variable(style)\n",
    "\n",
    "    content_image = Variable(utils.preprocess_batch(content_image))\n",
    "    style_model.setTarget(style_v)\n",
    "\n",
    "    output = style_model(content_image)\n",
    "    #output = utils.color_match(output, style_v)\n",
    "    utils.tensor_save_bgrimage(output.data[0], out_img)\n",
    "    print ('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Status (train/optimize/evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/torch/nn/modules/upsampling.py:122: UserWarning: nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "if mode == 'train':\n",
    "    print ('Training...')\n",
    "    train()\n",
    "elif mode == 'evaluate':\n",
    "    print ('Evaluating...')\n",
    "    evaluate(EV_MODEL_DIR, EV_CONT_IMG, EV_STYLE_IMG, IMAGE_SIZE, EV_OUTPUT_IMG)\n",
    "else:\n",
    "    print ('Error!!!!!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
